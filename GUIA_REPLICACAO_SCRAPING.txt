üìã GUIA DE REPLICA√á√ÉO - SISTEMA DE SCRAPING MAX TITANIUM EUROPA
==============================================================================

üéØ OBJETIVO: Documentar problemas, solu√ß√µes e melhores pr√°ticas para replicar 
            este sistema em outros 12 scrapers similares.

üìÖ CRIADO: Janeiro 2025
üñ•Ô∏è AMBIENTE: Linux CachyOS, Fish Shell, Python 3.13+

==============================================================================
üìä RESUMO DO PROJETO
==============================================================================

SITE ALVO: maxtitanium.eu (vers√£o europeia)
PRODUTOS: 20 produtos √∫nicos em 3 categorias
DESAFIO: Tabelas nutricionais dentro de dropdowns JavaScript
SOLU√á√ÉO: Selenium + m√∫ltiplas estrat√©gias de busca + detec√ß√£o autom√°tica

==============================================================================
üö® PROBLEMAS ENCONTRADOS E SOLU√á√ïES
==============================================================================

1. PROBLEMA: BeautifulSoup n√£o funcionava
   CAUSA: Conte√∫do carregado via JavaScript
   SOLU√á√ÉO: Migra√ß√£o para Selenium WebDriver
   C√ìDIGO: driver = webdriver.Chrome(options=chrome_options)

2. PROBLEMA: Dropdown "Informa√ß√£o Nutricional" n√£o encontrado
   CAUSA: Elemento pode estar com diferentes estruturas HTML
   SOLU√á√ÉO: M√∫ltiplas estrat√©gias de busca
   C√ìDIGO: 
   - Busca por h2 espec√≠fico
   - Busca por summary gen√©rico
   - Varredura completa da p√°gina

3. PROBLEMA: Clique interceptado por outros elementos
   CAUSA: Elemento n√£o vis√≠vel ou sobreposto
   SOLU√á√ÉO: Clique via JavaScript
   C√ìDIGO: driver.execute_script("arguments[0].click();", dropdown)

4. PROBLEMA: Elementos n√£o vis√≠veis na tela
   CAUSA: Dropdown pode estar fora da viewport
   SOLU√á√ÉO: Scroll autom√°tico inteligente
   C√ìDIGO: driver.execute_script("arguments[0].scrollIntoView(true);", element)

5. PROBLEMA: Configura√ß√£o manual de navegador
   CAUSA: Diferentes sistemas operacionais
   SOLU√á√ÉO: Detec√ß√£o autom√°tica de Chrome/Chromium
   C√ìDIGO: shutil.which("google-chrome") / shutil.which("chromium")

6. PROBLEMA: Feedback limitado durante execu√ß√£o
   CAUSA: subprocess.run() n√£o mostra progresso
   SOLU√á√ÉO: subprocess.Popen() com stdout em tempo real
   C√ìDIGO: processo.stdout.readline() em loop

7. PROBLEMA: Valores vazios no CSV
   CAUSA: Campos nutricionais n√£o encontrados
   SOLU√á√ÉO: Valores padr√£o 0 (conforme mem√≥ria do usu√°rio)
   C√ìDIGO: valor = extrair_valor(campo) or 0

8. PROBLEMA: Imagens diferentes no carousel
   CAUSA: Nomes de arquivo variam por produto
   SOLU√á√ÉO: Sempre selecionar segunda imagem (conforme mem√≥ria)
   C√ìDIGO: imagens[1].click() # Segunda imagem sempre

==============================================================================
üèóÔ∏è ARQUITETURA FINAL IMPLEMENTADA
==============================================================================

ESTRUTURA:
scraping_project/
‚îú‚îÄ‚îÄ main.py                      # Menu interativo principal
‚îú‚îÄ‚îÄ config/                      # M√≥dulos funcionais
‚îÇ   ‚îú‚îÄ‚îÄ browser.py               # Detec√ß√£o autom√°tica navegador
‚îÇ   ‚îú‚îÄ‚îÄ urls.py                  # Coletor URLs
‚îÇ   ‚îú‚îÄ‚îÄ teste.py                 # Teste individual
‚îÇ   ‚îú‚îÄ‚îÄ coleta.py                # Coleta dados nutricionais
‚îÇ   ‚îú‚îÄ‚îÄ teste_feedback.py        # Simula√ß√£o feedback
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt         # Depend√™ncias
‚îú‚îÄ‚îÄ dados/                       # Arquivos CSV
‚îî‚îÄ‚îÄ venv/                        # Ambiente virtual

MENU PRINCIPAL (7 op√ß√µes):
1. Coletar URLs
2. Teste individual
3. Coleta completa
4. Apenas dados nutricionais
5. Informa√ß√µes sistema
6. Teste feedback
7. Testar navegador

==============================================================================
üîß CONFIGURA√á√ïES T√âCNICAS CR√çTICAS
==============================================================================

CHROME OPTIONS (browser.py):
- --no-sandbox
- --disable-dev-shm-usage
- --disable-gpu
- --disable-extensions
- --disable-plugins
- --disable-images
- --disable-javascript (para performance)
- --window-size=1920,1080
- --headless (modo invis√≠vel)

ZOOM CONFIGUR√ÅVEL:
- URLs: 25% (ver mais produtos)
- Dados: 100% (leitura normal)
- C√≥digo: options.add_argument(f"--force-device-scale-factor={zoom_factor}")

DETEC√á√ÉO NAVEGADOR:
Linux: ["google-chrome", "google-chrome-stable", "chrome", "chromium", "chromium-browser"]
Windows: Caminhos padr√£o + Program Files
macOS: Suporte b√°sico

==============================================================================
üí° ESTRAT√âGIAS DE BUSCA IMPLEMENTADAS
==============================================================================

DROPOUT "INFORMA√á√ÉO NUTRICIONAL":

1. ESTRAT√âGIA ESPEC√çFICA:
   dropdown = driver.find_element(By.XPATH, "//h2[contains(text(), 'Informa√ß√£o Nutricional')]")

2. ESTRAT√âGIA GEN√âRICA:
   summaries = driver.find_elements(By.TAG_NAME, "summary")
   for summary in summaries:
       if "nutricional" in summary.text.lower():

3. ESTRAT√âGIA COMPLETA:
   all_elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'Informa√ß√£o Nutricional')]")

SCROLL INTELIGENTE:
- Rola p√°gina at√© encontrar elemento
- M√°ximo 10 tentativas
- Aguarda 1 segundo entre tentativas

==============================================================================
üì± FEEDBACK VISUAL IMPLEMENTADO
==============================================================================

SUBSTITUI√á√ÉO:
- ‚ùå subprocess.run() (sem feedback)
- ‚úÖ subprocess.Popen() (tempo real)

C√ìDIGO:
processo = subprocess.Popen(
    [sys.executable, script_path],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True,
    bufsize=1,
    universal_newlines=True
)

while True:
    output = processo.stdout.readline()
    if output == '' and processo.poll() is not None:
        break
    if output:
        print(output.strip())

MENSAGENS:
- üîç Processando produto X de Y
- ‚úÖ Sucesso / ‚ùå Erro
- üìä Dados extra√≠dos: X campos
- ‚è±Ô∏è Tempo estimado restante

==============================================================================
üéØ TEMPLATE PARA NOVOS SCRAPERS
==============================================================================

PASSOS PARA REPLICAR:

1. AN√ÅLISE INICIAL:
   - Verificar se site usa JavaScript
   - Identificar onde est√£o os dados alvo
   - Testar com BeautifulSoup primeiro

2. ESTRUTURA BASE:
   - Copiar estrutura de pastas
   - Adaptar main.py para novo site
   - Configurar browser.py

3. SCRIPTS PRINCIPAIS:
   - urls.py: Coletar URLs produtos
   - teste.py: Testar um produto
   - coleta.py: Processar todos

4. CONFIGURA√á√ïES:
   - Adaptar seletores CSS/XPath
   - Configurar delays espec√≠ficos
   - Ajustar zoom se necess√°rio

5. TESTES:
   - Sempre testar um produto primeiro
   - Verificar CSV gerado
   - Validar dados extra√≠dos

==============================================================================
üîç SELETORES E PATTERNS √öTEIS
==============================================================================

DROPDOWNS:
- "//details[contains(@class, 'dropdown')]"
- "//summary[contains(text(), 'Informa√ß√£o')]"
- "//h2[contains(text(), 'Nutricional')]"

TABELAS:
- "//table[@class='nutritional-table']"
- "//tr[contains(@class, 'nutri-row')]"
- "//td[contains(@class, 'nutri-value')]"

PRODUTOS:
- "//div[contains(@class, 'product-item')]"
- "//a[contains(@class, 'product-link')]"
- "//h3[contains(@class, 'product-title')]"

VALORES:
- re.findall(r'[\d,]+', texto)  # N√∫meros
- re.sub(r'[^\d,]', '', texto)  # Limpar
- float(valor.replace(',', '.'))  # Converter

==============================================================================
‚öôÔ∏è CONFIGURA√á√ïES POR SISTEMA
==============================================================================

LINUX:
- Caminho Chrome: /usr/bin/google-chrome
- ChromeDriver: /usr/bin/chromedriver
- Permissions: chmod +x scripts

WINDOWS:
- Caminho Chrome: C:\Program Files\Google\Chrome\Application\chrome.exe
- ChromeDriver: C:\chromedriver\chromedriver.exe
- Encoding: definir UTF-8

DEPEND√äNCIAS:
selenium==4.15.2
beautifulsoup4==4.12.2
requests==2.31.0
pandas==2.1.4 (opcional)

==============================================================================
üö´ ERROS COMUNS E SOLU√á√ïES
==============================================================================

ERRO: "element click intercepted"
SOLU√á√ÉO: driver.execute_script("arguments[0].click();", element)

ERRO: "element not found"
SOLU√á√ÉO: WebDriverWait + expected_conditions

ERRO: "chromedriver not found"
SOLU√á√ÉO: Detec√ß√£o autom√°tica ou webdriver-manager

ERRO: "window closed unexpectedly"
SOLU√á√ÉO: Adicionar delays e try/except

ERRO: "values not extracted"
SOLU√á√ÉO: M√∫ltiplas estrat√©gias + valores padr√£o

==============================================================================
üìà PERFORMANCE E OTIMIZA√á√ïES
==============================================================================

VELOCIDADE:
- Modo headless: 50% mais r√°pido
- Disable images: 30% mais r√°pido
- Disable JavaScript: 20% mais r√°pido (quando poss√≠vel)

DELAYS:
- Entre requests: 1-2 segundos
- Ap√≥s cliques: 0.5 segundos
- Carregamento p√°gina: 3-5 segundos

MEMORY:
- driver.quit() sempre no final
- Limpar cache: driver.delete_all_cookies()
- Restart driver a cada 50 produtos

==============================================================================
üéÆ COMANDOS √öTEIS PARA DESENVOLVIMENTO
==============================================================================

ATIVAR AMBIENTE:
source venv/bin/activate

INSTALAR DEPEND√äNCIAS:
pip install -r requirements.txt

EXECUTAR SISTEMA:
python main.py

TESTAR INDIVIDUAL:
python config/teste.py

TESTAR NAVEGADOR:
python config/browser.py

LIMPAR CACHE:
rm -rf __pycache__ config/__pycache__

==============================================================================
üìù CHECKLIST PARA NOVOS SCRAPERS
==============================================================================

ANTES DE COME√áAR:
‚ñ° Analisar estrutura do site alvo
‚ñ° Identificar uso de JavaScript
‚ñ° Verificar robots.txt
‚ñ° Testar rate limiting

DURANTE DESENVOLVIMENTO:
‚ñ° Implementar detec√ß√£o autom√°tica navegador
‚ñ° Criar m√∫ltiplas estrat√©gias de busca
‚ñ° Adicionar feedback visual
‚ñ° Configurar valores padr√£o
‚ñ° Testar em um produto primeiro

FINALIZA√á√ÉO:
‚ñ° Documentar seletores espec√≠ficos
‚ñ° Configurar delays apropriados
‚ñ° Testar coleta completa
‚ñ° Validar dados extra√≠dos
‚ñ° Criar README espec√≠fico

==============================================================================
üß† MEM√ìRIAS IMPORTANTES DO USU√ÅRIO
==============================================================================

1. Valores nutricionais n√£o encontrados = 0 (n√£o vazio)
2. Selenium sempre segunda imagem do carousel
3. Usu√°rio n√£o quer APIs pagas de OCR
4. Python vers√£o mais recente sendo usada
5. Prefer√™ncia por ferramentas gratuitas/open-source

==============================================================================
üéØ PR√ìXIMOS SCRAPERS - CONSIDERA√á√ïES
==============================================================================

ADAPTA√á√ïES NECESS√ÅRIAS:
- Seletores CSS/XPath espec√≠ficos
- Estrutura de dados diferente
- Delays espec√≠ficos do site
- Configura√ß√µes de zoom
- Estrat√©gias de busca √∫nicas

MANTER:
- Arquitetura de pastas
- Menu interativo
- Detec√ß√£o autom√°tica navegador
- Feedback visual
- Tratamento de erros

==============================================================================
üîó LINKS E REFER√äNCIAS
==============================================================================

Selenium Documentation: https://selenium-python.readthedocs.io/
BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
XPath Tester: https://www.freeformatter.com/xpath-tester.html
CSS Selector: https://www.w3schools.com/cssref/css_selectors.asp

==============================================================================
üìä M√âTRICAS DE SUCESSO
==============================================================================

ESTE PROJETO:
- 20 produtos processados
- 100% taxa de sucesso
- Tempo m√©dio: 2 minutos por produto
- 0 erros em coleta completa
- Detec√ß√£o autom√°tica funcionando

METAS PARA PR√ìXIMOS:
- Manter 95%+ taxa de sucesso
- Tempo < 3 minutos por produto
- 0 configura√ß√£o manual
- Feedback visual sempre ativo

==============================================================================
üìÖ HIST√ìRICO DE VERS√ïES
==============================================================================

v1.0 - Scripts b√°sicos BeautifulSoup
v2.0 - Migra√ß√£o Selenium
v3.0 - M√∫ltiplas estrat√©gias busca
v4.0 - Menu interativo + feedback
v5.0 - Detec√ß√£o autom√°tica navegador

==============================================================================
üéâ CONCLUS√ÉO
==============================================================================

Este guia cont√©m todas as informa√ß√µes necess√°rias para replicar o sistema 
de scraping em novos sites. Use como refer√™ncia para evitar problemas j√° 
resolvidos e acelerar o desenvolvimento dos pr√≥ximos 12 scrapers.

LEMBRE-SE: Sempre teste primeiro com um produto individual antes de 
executar coleta completa.

BOA SORTE! üöÄ

============================================================================== 